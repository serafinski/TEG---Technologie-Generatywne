import os
import io
import json
import PyPDF2
import logging
import requests

from dotenv import load_dotenv

# LangChain related imports
from langchain_core.documents import Document
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Ragas related imports
from ragas import EvaluationDataset, evaluate
from ragas.llms import LangchainLLMWrapper
from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness

# Suppress informational logs from httpx
logging.getLogger("httpx").setLevel(logging.WARNING)


def load_config():
    """
    Loads environment variables and validates the API key.
    :return: The OpenAI API key.
    """
    load_dotenv()
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("Missing OPENAI_API_KEY environment variable.")
    return openai_api_key


def download_and_extract_paper(paper_url):
    """
    Downloads a PDF from the given URL and extracts its text.
    :param paper_url: URL to the PDF.
    :return: Extracted text from the PDF.
    """
    response = requests.get(paper_url)
    if response.status_code != 200:
        raise ValueError(f"Failed to download paper. Status code: {response.status_code}")
    
    # Read PDF from the response content
    pdf_file = io.BytesIO(response.content)
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    paper_text = ""
    for page in pdf_reader.pages:
        paper_text += page.extract_text() + "\n"
    return paper_text


def create_documents(content_list):
    """
    Creates Document objects from a list of strings.
    :param content_list: List of strings representing document content.
    :return: List of Document objects.
    """
    return [Document(page_content=content) for content in content_list]


def init_vector_store(documents, api_key):
    """
    Initializes the embeddings and vector store, and adds documents to it.
    :param documents: List of Document objects.
    :param api_key: The OpenAI API key.
    :return: A retriever built from the vector store.
    """
    embeddings = OpenAIEmbeddings(api_key=api_key, model="text-embedding-3-small")
    vector_store = InMemoryVectorStore(embeddings)
    vector_store.add_documents(documents)
    return vector_store.as_retriever(search_kwargs={"k": 1})


def create_qa_chain(api_key):
    """
    Creates a Q&A chain using a prompt template, Chat LLM, and output parser.
    :param api_key: The OpenAI API key.
    :return: The Q&A chain and the underlying LLM instance.
    """
    llm = ChatOpenAI(api_key=api_key, model="gpt-4o-mini")
    template = (
        "Answer the question based only on the following context:\n"
        "{context}\n\n"
        "Question: {evaluation_query}\n"
    )
    prompt = ChatPromptTemplate.from_template(template)
    # The chain now expects a variable 'evaluation_query' instead of just 'query'
    return prompt | llm | StrOutputParser(), llm


def format_docs(relevant_docs):
    """
    Formats a list of Document objects into a single string.
    :param relevant_docs: List of Document objects.
    :return: Combined string of all document contents.
    """
    return "\n".join(doc.page_content for doc in relevant_docs)


def evaluate_single_query(evaluation_query, retriever, qa_chain):
    """
    Runs a single query through the retrieval and Q&A pipeline.
    :param evaluation_query: The query string.
    :param retriever: The document retriever.
    :param qa_chain: The Q&A chain.
    :return: The response generated by the chain.
    """
    relevant_docs = retriever.invoke(evaluation_query)
    response = qa_chain.invoke({"context": format_docs(relevant_docs), "evaluation_query": evaluation_query})
    logging.info("Evaluation Query: %s", evaluation_query)
    logging.info("Generated Response: %s", response)
    return response

def generate_qa_pairs():
    
    def load_qa_pairs(file_path):
        with open(file_path, 'r', encoding='utf-8') as file:
            return json.load(file)

    qa_pairs = load_qa_pairs('qa_pairs.json')
    return qa_pairs

def build_evaluation_dataset(evaluation_queries, ground_truth_answers, retriever, qa_chain):
    """
    Builds an evaluation dataset by processing evaluation queries and comparing responses to ground truth answers.
    :param evaluation_queries: List of query strings.
    :param ground_truth_answers: List of expected (ground truth) answers.
    :param retriever: The document retriever.
    :param qa_chain: The Q&A chain.
    :return: An EvaluationDataset instance.
    """
    dataset = []
    for evaluation_query, ground_truth_answer in zip(evaluation_queries, ground_truth_answers):
        relevant_docs = retriever.invoke(evaluation_query)
        generated_response = qa_chain.invoke({"context": format_docs(relevant_docs), "evaluation_query": evaluation_query})
        dataset.append({
            "user_input": evaluation_query,
            "retrieved_contexts": [doc.page_content for doc in relevant_docs],
            "response": generated_response,
            "reference": ground_truth_answer,
        })
    return EvaluationDataset.from_list(dataset)


def evaluate_dataset(evaluation_dataset, llm):
    """
    Evaluates the dataset using RAGAS metrics.
    :param evaluation_dataset: The dataset built for evaluation.
    :param llm: The language model wrapper.
    :return: The evaluation result.
    """
    evaluator_llm = LangchainLLMWrapper(llm)
    result = evaluate(
        dataset=evaluation_dataset,
        metrics=[
            # Jak bardzo sensowny jest kontekst
            LLMContextRecall(), 
            # Jakość odpowiedzi w porównaniu do kontekstu
            Faithfulness(), 
            # Jak bardzo odpowiedź jest zgodna z ground truth
            FactualCorrectness()
        ],
        llm=evaluator_llm,
    )
    logging.info("Evaluation Result: %s", result)
    return result


def main():
    # Set up logging
    logging.basicConfig(level=logging.INFO)

    # Step 1: Load Configuration
    openai_api_key = load_config()

    # Step 2: Download and Extract Paper Text
    paper_url = "https://arxiv.org/pdf/2503.18968"
    paper_text = download_and_extract_paper(paper_url)
    logging.info("Extracted paper text length: %d characters", len(paper_text))

    # Step 3: Create Document from Paper Text and Initialize Vector Store
    paper_document = create_documents([paper_text])
    retriever = init_vector_store(paper_document, openai_api_key)

    # Step 4: Generate Q&A Pairs Using a Separate Model
    qa_pairs = generate_qa_pairs()
    # Extract evaluation queries and ground truth answers from generated Q&A pairs
    evaluation_queries = [pair["question"] for pair in qa_pairs]
    ground_truth_answers = [pair["answer"] for pair in qa_pairs]
    logging.info("Generated %d Q&A pairs", len(qa_pairs))

    # Step 5: Create Q&A Chain for Retrieval-based Answering
    qa_chain, llm = create_qa_chain(openai_api_key)

    # Optional: Evaluate a single query for demonstration
    single_query = evaluation_queries[0]
    _ = evaluate_single_query(single_query, retriever, qa_chain)

    # Step 6: Build Evaluation Dataset from Generated Q&A Pairs
    evaluation_dataset = build_evaluation_dataset(evaluation_queries, ground_truth_answers, retriever, qa_chain)

    # Step 7: Evaluate the Dataset using RAGAS Metrics
    results = evaluate_dataset(evaluation_dataset, llm)
    print("Final Evaluation Results:")
    print(results)


if __name__ == "__main__":
    main()
